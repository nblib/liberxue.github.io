---
layout: blog
istop: false
title: "MLib学习之特征提取"
description: "spark机器学习,spark的MLib特征提取"
date:  2018-08-29
category: spark
tags:
- 大数据
- spark
- 机器学习
---

# 特征提取

MLib的特征提取主要针对文本的特征提取.包括如下三种:
* TF-IDF(逆文档频率): 统计单词对于标识一个文档的重要性,如果一个单词在单个文档中出现次数高,在其他文档中次数低,那么它的重要性也就高,也就是TF-IDF的值大
* Word2Vec: 将文档中的每个单词映射为一个向量,然后根据单词将文档转化为一个向量,可用于文档相似性计算等
* CountVectorizer: 将文本文档转换为标记数量的向量

## 特征提取简介

### 什么是特征

特征汉语释义为`一事物异于其他事物的特点`.比如一个物种懂得使用语言,会使用工具,那么可以很确定(不代表100%)这个物种就是人.
那么`懂得使用语言,会使用工具`可以说是人的一个特征.但是这个特征不能区分是男人还是女人还是其他性别.如果要区别性别,还需要更多的特征.
所以,在机器学习中特征用来:
* 从大量数据中不断抽取可以代表一个范围的特征.实现分类
* 不断提取特征,可以将大量数据,不断缩小.特征越来越详细,符合这个特征的数据越来越少

在机器学习中,分类也就是将一系列具有相同特征的数据归为一类.图像识别对比也就是不断提取图像中的特征,
使符合特征的数据越来越少,从而实现对比.

### 什么是特征提取

知道特征是什么了,接下来就要知道怎么提取特征.比如院子里有8只不同物种的动物.我们为了分析他们,首先需要描述他们,
而描述,就是说出他们的特点.这个过程就是提取特征.比如我们依次提取特征,来描述下:
首先提取一个特征
```
动物
动物这个特征很明显,八只动物都被包含了,要说它的特别,可能就是动物排除了院子
```
接下来基于这个特征再细化特征
```
哺乳 动物
具有这两个特征的有狗,猫,羊.这样其他的非哺乳动物就被排除了.这里就已经实现了一个简单的分类:哺乳动物和其他动物
```
通过不断的提取特征,分类越来越多,而每个分类中的内容则越来越少.

## mlib中的特征提取
mlib的特征提取主要针对文本的特征提取,文本的特征提取,也就是获取可以描述一个文本的特征.比如文本的关键字,文本中单词出现的数量等,通过这些
我们可以从茫茫文海中对文档进行分类,标识,查找等.

> 文档特征提取
>> 比如我看完一篇文章,想要告诉你,那么我会首先告诉你文章的`名字`,`讲了什么`,通过说这些可以让你更好的理解明白我看的
文章是什么,尽管不知道是具体哪个文章,单也基本知道了是关于什么范围的.而不至于我一说,你的脑海里是百科全书,完全不知道我说的是哪个范围的.

### TF-IDF（词频-逆向文档频率)

`词频-逆向文档频率` 包含两个步骤:`词频`和`逆向文档`:
* 词频: 统计一个词在文档中出现的次数.次数越多,说明这个词越有可能作为这个文档的特征
* 逆文档频率: 有的词在文档中出现次数很多,比如`is`,`a`,`你`,显然这些通常不能作为特征,他们通常在所有文档中出现次数大,如果某个词在所有文档中出现次数大,那么可以降低它的权重,这里就是计算逆文档频率

> 具体关于`TF-IDF`,可以查看`机器学习算法`相关的文档

在MLib中,适用于TF-IDF的工具:
* TF: HashingTF与CountVectorizer都可以用于生成词频TF向量。
* IDF: IDF是一个适合数据集并生成IDFModel的评估器

下面介绍演示下:
```
/**
    * hashingTF, 词频
    *
    * @param session
    */
  def HashingTF(session: SparkSession) = {
    import session.implicits._
    //模拟数据,包含两列,第一列为标签(id),这里没用,第二列为语句.
    val ratings = session.sparkContext.parallelize(Seq(
      (0.0, "Hi I yeard about Spark"),
      (1.0, "I wish Java could use case classes"),
      (2.0, "Logistic regression models are neat")
    )).toDF("label", "sentence")
    //打印结果,false表示显示完整的数据,而不是缩略
    ratings.show(false)

    //分词,将一个句子分割为单个单词.这里先不用管
    val tokenizer = new Tokenizer().setInputCol("sentence").setOutputCol("words")
    val wordsData = tokenizer.transform(df)

    //打印结果
    wordsData.show(false)

    //统计词频,hashingTF,通过对词进行hashing,生成hash值,同时进行计数.
    val hashingTF = new HashingTF()
      .setInputCol("words").setOutputCol("rawFeatures") //
    //.setNumFeatures(20) //设置hash表的桶数,数量过小会出现冲突,将不同的值统计成一个词.比如这里设置为20,会导致c开头的词被当成一个词进行统计

    //开始数据转换,将原始数据通过hashingtf转换为新的df
    val featurizedData = hashingTF.transform(wordsData)
    //打印结果
    featurizedData.show(false)

    //统计词频后,需要降低那些在所有文档中都出现的词语的权重,通过IDF进行处理
    val idf = new IDF().setInputCol("rawFeatures").setOutputCol("features")
    val idfModel = idf.fit(featurizedData)
    val rescaledData = idfModel.transform(featurizedData)
    //打印结果
    rescaledData.show(truncate = false)
  }
```
输出结果,上面每个阶段都进行了输出打印:
```
模拟数据后,打印如下,可以看到相当于一个数据表,包含两列,三行:
+-----+-----------------------------------+
|label|sentence                           |
+-----+-----------------------------------+
|0.0  |Hi I yeard about Spark             |
|1.0  |I wish Java could use case classes |
|2.0  |Logistic regression models are neat|
+-----+-----------------------------------+
通过分词器,分词后,生成words列,将句子分割为单个单词放到了一个数组中
+-----+-----------------------------------+------------------------------------------+
|label|sentence                           |words                                     |
+-----+-----------------------------------+------------------------------------------+
|0.0  |Hi I yeard about Spark             |[hi, i, yeard, about, spark]              |
|1.0  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|
|2.0  |Logistic regression models are neat|[logistic, regression, models, are, neat] |
+-----+-----------------------------------+------------------------------------------+
通过统计词频,可以看到多了一列rawFeatures,通过对列words中的每个单词进行hash,生成hash值,并统计数量.
rawFeatures值为一个元组,元组中第一个元素为hash桶大小,相当于一个数组的大小,第二个元素为数组,存放hash值,也就是在桶中的索引
第三个元素为数组,存放每个单词对应出现的数量.
+-----+-----------------------------------+------------------------------------------+--------------------------------------------------------------------------------------+
|label|sentence                           |words                                     |rawFeatures                                                                           |
+-----+-----------------------------------+------------------------------------------+--------------------------------------------------------------------------------------+
|0.0  |Hi I yeard about Spark             |[hi, i, yeard, about, spark]              |(262144,[24417,49304,91137,101780,234657],[1.0,1.0,1.0,1.0,1.0])                      |
|1.0  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|(262144,[20719,24417,55551,116873,147765,162369,192310],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|
|2.0  |Logistic regression models are neat|[logistic, regression, models, are, neat] |(262144,[13671,91006,132713,167122,190884],[1.0,1.0,1.0,1.0,1.0])                     |
+-----+-----------------------------------+------------------------------------------+--------------------------------------------------------------------------------------+

IDF根据生成的词频,减少在多个文档中出现的词的比重,比如i在第一条和第二条数据中都出现,那么它的比重就会比其他词的小
+-----+-----------------------------------+------------------------------------------+--------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|label|sentence                           |words                                     |rawFeatures                                                                           |features                                                                                                                                                                                        |
+-----+-----------------------------------+------------------------------------------+--------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|0.0  |Hi I yeard about Spark             |[hi, i, yeard, about, spark]              |(262144,[24417,49304,91137,101780,234657],[1.0,1.0,1.0,1.0,1.0])                      |(262144,[24417,49304,91137,101780,234657],[0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                    |
|1.0  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|(262144,[20719,24417,55551,116873,147765,162369,192310],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|(262144,[20719,24417,55551,116873,147765,162369,192310],[0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])|
|2.0  |Logistic regression models are neat|[logistic, regression, models, are, neat] |(262144,[13671,91006,132713,167122,190884],[1.0,1.0,1.0,1.0,1.0])                     |(262144,[13671,91006,132713,167122,190884],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                    |
+-----+-----------------------------------+------------------------------------------+--------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
```

##### 个别方法介绍
> show(false)
>> 打印DF中的数据,false表示打印完整数据,如果为true那么打印的数据可能是这样: `I am a pr...`,出现了省略,设为false的话,那么久打印完整的数据:`I am a programmer`

> setInputCol("sentence").setOutputCol("words")
>> 设置输入数据的列和输出的列.也就是指定要对哪一列进行处理,然后输出处理结果到哪个列.可以看上面的输出结果

> hashingTF.setNumFeatures(20)
>> 设置hash桶的大小.比如上面的数据如果设置桶大小为20,那么会出现所有c开头的词的hash值一样,被当成一个词进行梳理统计了,所以这个值最好根据词库设置大些,默认为2^20 = 1,048,576

TODO: Word2Vec...