---
layout: blog
istop: true
title: "MLib学习之管道"
description: "spark机器学习,spark的MLib管道"
date:  2018-08-27
category: spark
tags:
- 大数据
- spark
- 机器学习
---
# ML Pipelines（ML管道）
`ML Pipelines`将数据处理,算法学习等机器学习步骤组织起来,形成一个工作流程.相关的概念:
* DataFrame（数据模型): DataFrame是通过spark sql加载的数据,比如从hive加载的数据,作为ML的数据集.
* Transformer（转换器): Transformer用于将一个DF(DataFrame)通过某种转化,生成一个新的DF.
* Estimator（模型学习器): Estimator 通过适配 DataFrame 来生成 Transformer,也就是对数据进行训练
* Pipeline（管道): Pipline 将多个转化器和Estimator组织起来,生成一个工作流
* Parameter(参数): 给Transtfermer和Estimator指定参数

ML一次机器学习的执行流程:
* 加载数据集,生成DataFrame
* 初始化Transformer和Estimator的对象
* 初始化一个Pipeline,也就是定义一个工作流.然后将Transformer和Estimator按照执行顺序添加到工作流中.
* 将DataFrame传给Pipeline,如果Transformer和Estimator需要参数,同时将参数传给Pipeline.
* 开始执行Pipeline.
* 执行完Pipeline后,生成一个模型.即为最终结果,可以用来测试未知数据.

## Pipelines 组件

### Transformers（转换器）
Transformer 是一个转化器.将一个DF通过Transformer定义的规则转化为另一个DF,比如:
* Tokenizer(分词器)是一个转化器,将一个DF中的每条数据中的一个句子分割为单个单词,然后生成一个新的DF
* 一个算法本质上也是通过训练生成一个规则,然后按照这个规则将一个DF转化为一个新的DF.达到预测或分类的算法效果

### Estimators（模型学习器）
通过上面的Transformer介绍,我们知道一个算法本质上是通过对训练数据的拟合,生成一个规则,然后按照这个规则对未知数据
进行转化,转化为新的DF.Estimator就是用来生成规则,也就是生成一个有指定规则的transformer: Model,比如:
* LogisticRegression(逻辑回归)是一个Estimator,通过这个学习器,对数据进行学习,也就是训练,生成了针对这个数据集的
规则的转化器.当使用时,使用这个生成的转化器对新的数据进行转化,即得到分类的结果.

## 举例
```
def main(args: Array[String]): Unit = {
    //创建基于DataFrame的DF
    val session = SparkSession.builder().master("local").appName("myApp").getOrCreate()
    import session.implicits._
    val ratings = session.sparkContext.parallelize(Seq(
      (0L, "a b c d e spark", 1.0),
      (1L, "b d", 0.0),
      (2L, "spark f g h", 1.0),
      (3L, "hadoop mapreduce", 0.0)
    )).toDF("id", "text", "label")
    ratings.show()

    //定义一个分词器,是一个transformer
    val tokenizer = new Tokenizer()
      .setInputCol("text")
      .setOutputCol("words")
    //定义一个词频计算器,是一个transformer
    val hashingTF = new HashingTF()
      .setNumFeatures(1000)
      .setInputCol(tokenizer.getOutputCol)
      .setOutputCol("features")
    //逻辑回归算法,一个estimator
    val lr = new LogisticRegression()
      .setMaxIter(10)
      .setRegParam(0.001)
    //定义一个pipeline,也就是一个工作流程,描述整个执行过程
    //将要执行的transformer和Estimator,按照执行顺序添加到pipeline中
    val pipeline = new Pipeline()
      .setStages(Array(tokenizer, hashingTF, lr))

    // 传入训练数据,开始工作流程,最后返回训练好的模型,也就是一个transformer.
    val model = pipeline.fit(ratings)
    //可以将这个transformer保存起来,方便使用
    model.write.overwrite().save("./tmp/spark-logistic-regression-model")

    // 也可以直接保存未训练的pipeline,下次使用时,直接加载
    pipeline.write.overwrite().save("./tmp/unfit-lr-model")
    //Pipeline.load("./tmp/unfit-lr-model")

    //加载一个训练好的模型
    val sameModel = PipelineModel.load("./tmp/spark-logistic-regression-model")

    //定义一个测试数据集
    val testDF = session.sparkContext.parallelize(Seq(
      (4L, "spark i j k"),
      (5L, "l m n"),
      (6L, "spark hadoop spark"),
      (7L, "apache hadoop")
    )).toDF("id", "text")
    //使用训练好的模型,即transformer,对测试数据进行转化
    sameModel.transform(testDF)
      .select("id", "text", "probability", "prediction")
      .collect()
      .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =>
        println(s"($id, $text) --> prob=$prob, prediction=$prediction")
      }

  }
```
通过这个例子,了解下pipeline和组件的使用.具体transformer和Estimator,将在后续详细讲解.

 

